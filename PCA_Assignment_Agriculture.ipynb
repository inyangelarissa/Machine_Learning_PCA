{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formative Assignment: Advanced Linear Algebra (PCA)\n",
    "This notebook will guide you through the implementation of Principal Component Analysis (PCA). Fill in the missing code and provide the required answers in the appropriate sections. You will work with a dataset that is Africanized.\n",
    "\n",
    "**Student Name:** Completed Example\n",
    "\n",
    "**Dataset:** African Agriculture and Food Security Indicators\n",
    "\n",
    "Make sure to display the output for each code cell after running it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries and Load African Dataset\n",
    "First, we'll import necessary libraries and create our Africanized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create African Agriculture and Food Security Dataset\n",
    "# This dataset contains agricultural and food security indicators for 45 African countries\n",
    "\n",
    "countries = [\n",
    "    'Nigeria', 'Ethiopia', 'Egypt', 'Democratic Republic of Congo', 'Tanzania',\n",
    "    'South Africa', 'Kenya', 'Uganda', 'Algeria', 'Sudan',\n",
    "    'Morocco', 'Angola', 'Ghana', 'Mozambique', 'Madagascar',\n",
    "    'Cameroon', \"Côte d'Ivoire\", 'Niger', 'Burkina Faso', 'Mali',\n",
    "    'Malawi', 'Zambia', 'Somalia', 'Senegal', 'Chad',\n",
    "    'Zimbabwe', 'Guinea', 'Rwanda', 'Benin', 'Burundi',\n",
    "    'Tunisia', 'Togo', 'Sierra Leone', 'Libya', 'Liberia',\n",
    "    'Mauritania', 'Central African Republic', 'Eritrea', 'Lesotho', 'Namibia',\n",
    "    'Botswana', 'Gabon', 'Gambia', 'Mauritius', 'Eswatini'\n",
    "]\n",
    "\n",
    "climate_zones = ['Arid', 'Semi-Arid', 'Tropical', 'Sub-Tropical', 'Mediterranean']\n",
    "n = len(countries)\n",
    "\n",
    "# Generate realistic correlated agricultural data\n",
    "# Base productivity that will influence other variables\n",
    "base_productivity = np.random.gamma(5, 2, n)\n",
    "\n",
    "data = {\n",
    "    'Country': countries,\n",
    "    'Climate_Zone': np.random.choice(climate_zones, n),\n",
    "    'Cereal_Production_MT': base_productivity * np.random.lognormal(2, 0.5, n),\n",
    "    'Arable_Land_Percent': np.random.beta(3, 5, n) * 100,\n",
    "    'Agricultural_Employment_Percent': np.random.beta(6, 2, n) * 100,\n",
    "    'Fertilizer_Use_KG_Per_Hectare': np.random.gamma(3, 15, n),\n",
    "    'Irrigation_Percent': np.random.beta(2, 8, n) * 100,\n",
    "    'Crop_Yield_Index': base_productivity * np.random.normal(100, 20, n),\n",
    "    'Food_Production_Index': base_productivity * np.random.normal(110, 25, n),\n",
    "    'Livestock_Production_Index': np.random.gamma(8, 12, n),\n",
    "    'Undernourishment_Percent': np.random.beta(3, 4, n) * 50,\n",
    "    'Agricultural_Value_Added_Percent_GDP': np.random.beta(4, 2, n) * 60,\n",
    "    'Rural_Population_Percent': np.random.beta(5, 3, n) * 100,\n",
    "    'Access_To_Electricity_Rural_Percent': np.random.beta(3, 5, n) * 100,\n",
    "    'Agricultural_Machinery_Per_100Sq_KM': np.random.gamma(2, 8, n),\n",
    "    'Food_Import_Dependency_Percent': np.random.beta(3, 4, n) * 80\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce missing values (REQUIREMENT: dataset must have missing values)\n",
    "missing_columns = ['Fertilizer_Use_KG_Per_Hectare', 'Irrigation_Percent', 'Undernourishment_Percent',\n",
    "                   'Access_To_Electricity_Rural_Percent', 'Food_Import_Dependency_Percent']\n",
    "\n",
    "for col in missing_columns:\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.18 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, col] = np.nan\n",
    "\n",
    "print(\"African Agriculture and Food Security Dataset Created!\")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full dataset overview\n",
    "print(\"Complete Dataset Overview:\")\n",
    "print(\"=\" * 70)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (REQUIRED)\n",
    "print(\"Missing Values Summary:\")\n",
    "print(\"=\" * 50)\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Check data types (REQUIRED: at least 1 non-numeric column)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Data Types:\")\n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes)\n",
    "print(f\"\\nNon-numeric columns: {df.select_dtypes(include=['object']).columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: Handle missing values and separate numeric data\n",
    "print(\"Data Preprocessing...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numeric columns: {numeric_cols}\")\n",
    "print(f\"\\nNumber of numeric features: {len(numeric_cols)}\")\n",
    "\n",
    "# Create a copy with only numeric columns for PCA\n",
    "df_numeric = df[numeric_cols].copy()\n",
    "\n",
    "# Handle missing values using mean imputation\n",
    "print(\"\\nHandling missing values using mean imputation...\")\n",
    "for col in df_numeric.columns:\n",
    "    if df_numeric[col].isnull().any():\n",
    "        mean_value = df_numeric[col].mean()\n",
    "        df_numeric[col].fillna(mean_value, inplace=True)\n",
    "        print(f\"  - Filled {col} with mean: {mean_value:.2f}\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\nMissing values after imputation: {df_numeric.isnull().sum().sum()}\")\n",
    "\n",
    "# Convert to numpy array for PCA implementation\n",
    "data_array = df_numeric.values\n",
    "\n",
    "print(f\"\\nData array shape: {data_array.shape}\")\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Standardize the Data\n",
    "Before applying PCA, we must standardize the dataset. Standardization ensures that all features have a mean of 0 and a standard deviation of 1, which is essential for PCA.\n",
    "Fill in the code to standardize the dataset.\n",
    "\n",
    "STRICTLY - Write code that implements standardization based on the formula:\n",
    "\n",
    "**Standardized Data = (Data - Mean) / Standard Deviation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Standardize the data (use of numpy only allowed)\n",
    "# Formula: standardized_data = (data - mean) / std\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(data_array, axis=0)\n",
    "std = np.std(data_array, axis=0)\n",
    "\n",
    "# Standardize the data: (Data - Mean) / Standard Deviation\n",
    "standardized_data = (data_array - mean) / std\n",
    "\n",
    "print(\"Data Standardization Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original data shape: {data_array.shape}\")\n",
    "print(f\"Standardized data shape: {standardized_data.shape}\")\n",
    "print(f\"\\nStandardized data mean (should be ~0): {np.mean(standardized_data, axis=0)[:5]}\")\n",
    "print(f\"Standardized data std (should be ~1): {np.std(standardized_data, axis=0)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Display Sample of Standardized Data\n",
    "Let's visualize what standardization did to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display standardized data\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=df_numeric.columns)\n",
    "print(\"Sample of Standardized Data:\")\n",
    "print(\"=\" * 70)\n",
    "standardized_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the Covariance Matrix\n",
    "The covariance matrix helps us understand how the features are related to each other. It is a key component in PCA.\n",
    "\n",
    "**Formula: Covariance Matrix = (1/(n-1)) × X^T × X**\n",
    "\n",
    "where X is the standardized data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the Covariance Matrix\n",
    "# Formula: cov_matrix = (1/(n-1)) * X^T * X\n",
    "\n",
    "# Get number of samples\n",
    "n_samples = standardized_data.shape[0]\n",
    "\n",
    "# Calculate covariance matrix using numpy\n",
    "cov_matrix = np.dot(standardized_data.T, standardized_data) / (n_samples - 1)\n",
    "\n",
    "print(\"Covariance Matrix Calculated!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Covariance matrix shape: {cov_matrix.shape}\")\n",
    "print(f\"\\nFirst 5x5 section of covariance matrix:\")\n",
    "print(cov_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the covariance matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cov_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1,\n",
    "            xticklabels=df_numeric.columns,\n",
    "            yticklabels=df_numeric.columns)\n",
    "plt.title('Covariance Matrix Heatmap - African Agriculture Dataset', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive values (red): Features move together in the same direction\")\n",
    "print(\"- Negative values (blue): Features move in opposite directions\")\n",
    "print(\"- Diagonal values: Variance of each feature (should be ~1 for standardized data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perform Eigendecomposition\n",
    "Eigendecomposition of the covariance matrix will give us the eigenvalues and eigenvectors, which are essential for PCA.\n",
    "Fill in the code to compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "**Formula: C × v = λ × v**\n",
    "\n",
    "where:\n",
    "- C is the covariance matrix\n",
    "- λ (lambda) are the eigenvalues\n",
    "- v are the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform Eigendecomposition\n",
    "# Use numpy's linalg.eig function to compute eigenvalues and eigenvectors\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Convert to real values (remove any imaginary components due to numerical precision)\n",
    "eigenvalues = eigenvalues.real\n",
    "eigenvectors = eigenvectors.real\n",
    "\n",
    "print(\"Eigendecomposition Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Number of eigenvalues: {len(eigenvalues)}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "print(f\"\\nFirst 5 eigenvalues: {eigenvalues[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sort Principal Components\n",
    "Sort the eigenvectors based on their corresponding eigenvalues in descending order. The higher the eigenvalue, the more important the eigenvector.\n",
    "Complete the code to sort the eigenvectors and print the sorted components.\n",
    "\n",
    "**Why sort?** The eigenvalue represents the amount of variance explained by its corresponding eigenvector (principal component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Sort Principal Components\n",
    "# Sort eigenvalues in descending order and reorder eigenvectors accordingly\n",
    "\n",
    "# Get indices that would sort eigenvalues in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "# Sort eigenvalues in descending order\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "\n",
    "# Reorder eigenvectors based on sorted eigenvalues\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "print(\"Principal Components Sorted!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Sorted eigenvalues (variance explained by each component):\")\n",
    "for i, eigenvalue in enumerate(sorted_eigenvalues):\n",
    "    print(f\"PC{i+1}: {eigenvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculate Explained Variance\n",
    "Calculate the proportion of variance explained by each principal component and the cumulative variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Calculate Explained Variance\n",
    "# Calculate explained variance ratio\n",
    "total_variance = np.sum(sorted_eigenvalues)\n",
    "explained_variance_ratio = sorted_eigenvalues / total_variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "variance_df = pd.DataFrame({\n",
    "    'Principal Component': [f'PC{i+1}' for i in range(len(sorted_eigenvalues))],\n",
    "    'Eigenvalue': sorted_eigenvalues,\n",
    "    'Variance Explained (%)': explained_variance_ratio * 100,\n",
    "    'Cumulative Variance (%)': cumulative_variance * 100\n",
    "})\n",
    "\n",
    "print(\"Explained Variance by Principal Components:\")\n",
    "print(\"=\" * 70)\n",
    "variance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scree plot\n",
    "ax1.bar(range(1, len(sorted_eigenvalues) + 1), explained_variance_ratio * 100, \n",
    "        alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Principal Component', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Variance Explained (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Scree Plot - Variance Explained by Each PC', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative variance plot\n",
    "ax2.plot(range(1, len(sorted_eigenvalues) + 1), cumulative_variance * 100, \n",
    "         marker='o', linewidth=2, markersize=8, color='darkgreen')\n",
    "ax2.axhline(y=80, color='red', linestyle='--', linewidth=2, label='80% Threshold')\n",
    "ax2.axhline(y=90, color='orange', linestyle='--', linewidth=2, label='90% Threshold')\n",
    "ax2.set_xlabel('Number of Principal Components', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Cumulative Variance Explained (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 80% and 90% variance\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nNumber of components needed for 80% variance: {n_components_80}\")\n",
    "print(f\"Number of components needed for 90% variance: {n_components_90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Project Data onto Principal Components\n",
    "Now that we have the principal components, project the standardized data onto them to get the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Project data onto principal components\n",
    "# Select number of components (let's use enough for 90% variance)\n",
    "n_components = n_components_90\n",
    "\n",
    "# Select the top n_components eigenvectors\n",
    "principal_components = sorted_eigenvectors[:, :n_components]\n",
    "\n",
    "# Project the data: Transformed Data = Standardized Data × Principal Components\n",
    "transformed_data = np.dot(standardized_data, principal_components)\n",
    "\n",
    "print(f\"Data Projection Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original data shape: {standardized_data.shape}\")\n",
    "print(f\"Transformed data shape: {transformed_data.shape}\")\n",
    "print(f\"Dimensionality reduction: {standardized_data.shape[1]} → {transformed_data.shape[1]} features\")\n",
    "print(f\"Variance retained: {cumulative_variance[n_components-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with transformed data\n",
    "transformed_df = pd.DataFrame(\n",
    "    transformed_data,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)]\n",
    ")\n",
    "transformed_df['Country'] = df['Country'].values\n",
    "\n",
    "print(\"Transformed Data (First 10 countries):\")\n",
    "print(\"=\" * 70)\n",
    "transformed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data in PC space (2D projection)\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(transformed_data[:, 0], transformed_data[:, 1], \n",
    "                     c=range(len(countries)), cmap='viridis', \n",
    "                     s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "\n",
    "# Annotate some countries\n",
    "for i, country in enumerate(countries[::5]):  # Label every 5th country to avoid clutter\n",
    "    plt.annotate(country, (transformed_data[i*5, 0], transformed_data[i*5, 1]),\n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}% variance)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}% variance)', fontsize=12, fontweight='bold')\n",
    "plt.title('African Countries in Principal Component Space', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Country Index')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analyze Component Loadings\n",
    "Understand which original features contribute most to each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loadings matrix (correlation between original features and PCs)\n",
    "loadings = sorted_eigenvectors[:, :n_components] * np.sqrt(sorted_eigenvalues[:n_components])\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "    index=df_numeric.columns\n",
    ")\n",
    "\n",
    "print(\"Component Loadings (Feature Contributions):\")\n",
    "print(\"=\" * 70)\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loadings for first 3 PCs\n",
    "fig, axes = plt.subplots(1, min(3, n_components), figsize=(18, 6))\n",
    "if n_components == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(min(3, n_components)):\n",
    "    ax = axes[i]\n",
    "    loadings_sorted = loadings_df[f'PC{i+1}'].sort_values()\n",
    "    colors = ['red' if x < 0 else 'green' for x in loadings_sorted]\n",
    "    loadings_sorted.plot(kind='barh', ax=ax, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Loading Value', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'PC{i+1} Loadings ({explained_variance_ratio[i]*100:.1f}% variance)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive loadings (green): Feature increases with PC\")\n",
    "print(\"- Negative loadings (red): Feature decreases with PC\")\n",
    "print(\"- Larger absolute values: Stronger contribution to the PC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Dynamic Component Selection Based on Variance Threshold\n",
    "Implement a function that dynamically selects the number of components based on a desired variance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Dynamic component selection\n",
    "def select_components_by_variance(eigenvalues, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Select the minimum number of components needed to explain\n",
    "    at least variance_threshold of the total variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eigenvalues : array\n",
    "        Sorted eigenvalues in descending order\n",
    "    variance_threshold : float\n",
    "        Desired proportion of variance to retain (default: 0.95)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    n_components : int\n",
    "        Number of components needed\n",
    "    \"\"\"\n",
    "    # Calculate total variance\n",
    "    total_var = np.sum(eigenvalues)\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    var_ratio = eigenvalues / total_var\n",
    "    \n",
    "    # Calculate cumulative variance\n",
    "    cumulative_var = np.cumsum(var_ratio)\n",
    "    \n",
    "    # Find the number of components needed\n",
    "    n_components = np.argmax(cumulative_var >= variance_threshold) + 1\n",
    "    \n",
    "    return n_components\n",
    "\n",
    "# Test the function with different thresholds\n",
    "print(\"Dynamic Component Selection:\")\n",
    "print(\"=\" * 70)\n",
    "for threshold in [0.80, 0.85, 0.90, 0.95, 0.99]:\n",
    "    n = select_components_by_variance(sorted_eigenvalues, threshold)\n",
    "    print(f\"Variance threshold {threshold*100:.0f}%: {n} components needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Performance Optimization and Reconstruction\n",
    "Implement data reconstruction from principal components and calculate reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Data reconstruction and error calculation\n",
    "def reconstruct_data(transformed_data, principal_components, mean, std):\n",
    "    \"\"\"\n",
    "    Reconstruct original data from principal components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transformed_data : array\n",
    "        Data in PC space\n",
    "    principal_components : array\n",
    "        Eigenvectors used for transformation\n",
    "    mean : array\n",
    "        Original data mean (for un-standardizing)\n",
    "    std : array\n",
    "        Original data standard deviation (for un-standardizing)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    reconstructed_data : array\n",
    "        Reconstructed original data\n",
    "    \"\"\"\n",
    "    # Inverse transform: PC Space → Standardized Space\n",
    "    standardized_reconstructed = np.dot(transformed_data, principal_components.T)\n",
    "    \n",
    "    # Un-standardize: Standardized → Original Scale\n",
    "    reconstructed_data = (standardized_reconstructed * std) + mean\n",
    "    \n",
    "    return reconstructed_data\n",
    "\n",
    "# Reconstruct data using different numbers of components\n",
    "print(\"Data Reconstruction Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for n in [2, 5, n_components_90, len(sorted_eigenvalues)]:\n",
    "    # Project data\n",
    "    pcs = sorted_eigenvectors[:, :n]\n",
    "    projected = np.dot(standardized_data, pcs)\n",
    "    \n",
    "    # Reconstruct\n",
    "    reconstructed = reconstruct_data(projected, pcs, mean, std)\n",
    "    \n",
    "    # Calculate reconstruction error (Mean Squared Error)\n",
    "    mse = np.mean((data_array - reconstructed) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate R² score\n",
    "    ss_tot = np.sum((data_array - np.mean(data_array, axis=0)) ** 2)\n",
    "    ss_res = np.sum((data_array - reconstructed) ** 2)\n",
    "    r2_score = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    print(f\"\\nComponents: {n}\")\n",
    "    print(f\"  Variance explained: {cumulative_variance[n-1]*100:.2f}%\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R² Score: {r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction quality\n",
    "n_test = 5  # Use 5 components for visualization\n",
    "pcs_test = sorted_eigenvectors[:, :n_test]\n",
    "projected_test = np.dot(standardized_data, pcs_test)\n",
    "reconstructed_test = reconstruct_data(projected_test, pcs_test, mean, std)\n",
    "\n",
    "# Compare original vs reconstructed for first 3 features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(data_array[:, i], reconstructed_test[:, i], alpha=0.6, s=50)\n",
    "    ax.plot([data_array[:, i].min(), data_array[:, i].max()],\n",
    "           [data_array[:, i].min(), data_array[:, i].max()],\n",
    "           'r--', linewidth=2, label='Perfect Reconstruction')\n",
    "    ax.set_xlabel(f'Original {df_numeric.columns[i]}', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(f'Reconstructed {df_numeric.columns[i]}', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{df_numeric.columns[i]}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Original vs Reconstructed Data ({n_test} Components)', \n",
    "            fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Analysis: Interpretation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL PCA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: African Agriculture and Food Security Indicators\")\n",
    "print(f\"Countries analyzed: {len(countries)}\")\n",
    "print(f\"Original features: {len(df_numeric.columns)}\")\n",
    "print(f\"\\nDimensionality Reduction:\")\n",
    "print(f\"  - {n_components_80} components explain 80% of variance\")\n",
    "print(f\"  - {n_components_90} components explain 90% of variance\")\n",
    "print(f\"  - {len(sorted_eigenvalues)} components explain 100% of variance\")\n",
    "print(f\"\\nTop 3 Principal Components:\")\n",
    "for i in range(min(3, len(sorted_eigenvalues))):\n",
    "    print(f\"  PC{i+1}: {explained_variance_ratio[i]*100:.2f}% variance\")\n",
    "    top_features = loadings_df[f'PC{i+1}'].abs().nlargest(3)\n",
    "    print(f\"       Top features: {', '.join(top_features.index.tolist())}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  - PCA successfully reduced dimensionality from {len(df_numeric.columns)} to {n_components_90} features\")\n",
    "print(f\"  - Retained {cumulative_variance[n_components_90-1]*100:.1f}% of original information\")\n",
    "print(f\"  - Most variance captured by first few components (as expected)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: 3D Visualization of Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization of first 3 principal components\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(transformed_data[:, 0], \n",
    "                    transformed_data[:, 1], \n",
    "                    transformed_data[:, 2],\n",
    "                    c=range(len(countries)), \n",
    "                    cmap='viridis',\n",
    "                    s=100, \n",
    "                    alpha=0.6, \n",
    "                    edgecolors='black',\n",
    "                    linewidth=1)\n",
    "\n",
    "# Label some countries\n",
    "for i in range(0, len(countries), 7):  # Label every 7th country\n",
    "    ax.text(transformed_data[i, 0], \n",
    "           transformed_data[i, 1], \n",
    "           transformed_data[i, 2],\n",
    "           countries[i], \n",
    "           fontsize=8)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)', fontsize=11, fontweight='bold')\n",
    "ax.set_zlabel(f'PC3 ({explained_variance_ratio[2]*100:.1f}%)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('3D Principal Component Space - African Agriculture', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Country Index', shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment, you have successfully:\n",
    "\n",
    "1. ✅ Loaded and preprocessed an Africanized dataset with missing values and non-numeric columns\n",
    "2. ✅ Implemented data standardization from scratch using numpy\n",
    "3. ✅ Calculated the covariance matrix manually\n",
    "4. ✅ Performed eigendecomposition to extract principal components\n",
    "5. ✅ Sorted components by explained variance\n",
    "6. ✅ Projected data onto principal component space\n",
    "7. ✅ Implemented dynamic component selection based on variance threshold\n",
    "8. ✅ Reconstructed data and analyzed reconstruction error\n",
    "9. ✅ Visualized results using multiple plotting techniques\n",
    "\n",
    "**Key Takeaways:**\n",
    "- PCA is a powerful dimensionality reduction technique\n",
    "- Standardization is crucial before applying PCA\n",
    "- First few components typically capture most variance\n",
    "- Component loadings help interpret what each PC represents\n",
    "- Trade-off between dimensionality reduction and information loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
